{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community. vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"]= GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os. environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97243884",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "llm = ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WebBaseLoader(url).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187415e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].metadata[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-prompt-engineering/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c66f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100\n",
    "    chunk_overlap=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits = splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfe7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e40fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retriever_blog_posts\",\n",
    "    description=\"Search and return information about Lilian Weng blog posts on LLM agent and prompt engineering\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b520ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_assistant(state: AgentState):\n",
    "    print(\"--- CALL AGENT ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    print(f\"This is my message: {messages}\")\n",
    "\n",
    "    if len(messages) > 1:\n",
    "        last_message = messages[-1]\n",
    "        question = last_message.content\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "\n",
    "            You are a helpful assistant, whatever question has been asked to find out that in the given question and then answer.\n",
    "            Here is the question: {question}\n",
    "            \n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"]\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "\n",
    "        response = chain.invoke({\"question\": question})\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    else:\n",
    "        llm_with_tool = llm.bind_tools(tools=tools)\n",
    "        response = llm_with_tool.invoke(messages)\n",
    "        # response = handel_query(messages)\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grade(BaseModel):\n",
    "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: AgentState) -> Literal[\"Output_Generator\", \"Query_Rewritor\"]:\n",
    "    llm_with_structure_output = llm.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "\n",
    "            You are a grader deciding if a document is relevant to user's question.\n",
    "            Here is the document: {context}\n",
    "            Here is the user's question: {question}\n",
    "            If the document talks about or contains information related to the user's question, mark it as relevant.\n",
    "            Give a 'yes' or 'no' answer to show if the document is relevant to the question\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_structure_output\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    print(f\"message from the grader: {messages}\")\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"Yes\":\n",
    "        print(\"--- DECISION: DOCS RELEVANT ---\")\n",
    "        return \"generator\" # This should be a node name\n",
    "    \n",
    "    else:\n",
    "        print(\"--- DECISION: DOCS NOT RELEVANT ---\")\n",
    "        return \"rewriter\" # This should be a node name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: AgentState):\n",
    "    print(\"--- GENERATE ---\")\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    print(f\"Here is message from generate: {messages}\")\n",
    "\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    rag_chain = prompt | llm\n",
    "    \n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    print(f\"This is my response: {response}\")\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state:AgentState):\n",
    "    print(\"--- TRANSFORM QUERY ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    print(f\"Here is message from rewrite: {messages}\")\n",
    "\n",
    "    message = [HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent or meaning.\n",
    "                            Here is the initial question: {question}\n",
    "                            Formulate an improved question:\"\"\")]\n",
    "    \n",
    "\n",
    "    response = llm.invoke(message)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"My_AI_Assistant\", ai_assistant)\n",
    "workflow.add_node(\"Vectore_Retriever\", retrieve)\n",
    "workflow.add_node(\"Output_Generator\", generate)\n",
    "workflow.add_node(\"Query_Rewriter\", rewrite)\n",
    "\n",
    "workflow.add_edge(START, \"MY_AI_Assistant\")\n",
    "\n",
    "workflow.add_conditional_edges(\"MY_AI_Assistant\", tools_condition, {\"tools\":\"Vector_Retriever\",END:END})\n",
    "\n",
    "workflow.add_conditional_edges(\"Vector_Retriever\",\n",
    "                               grade_documents,\n",
    "                               {\"generator\": \"Output_Generator\",\n",
    "                                \"rewritor\": \"Query_Rewritor\"})\n",
    "workflow.add_edge(\"Output_Generator\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ad6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a898e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\": [\"What is Autonomous Agent?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\": [\"What is the Capital of USA?\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
