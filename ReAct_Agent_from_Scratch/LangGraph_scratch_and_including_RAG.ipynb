{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "from IPython.display import Image, display\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee95ef8",
   "metadata": {},
   "source": [
    "# This is a simple workflow with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    return input_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \"from first function\"\n",
    "\n",
    "def function_2(input_2):\n",
    "\n",
    "    output = function_3(\"This is function 3 in between\")\n",
    "\n",
    "    return input_2 + \" \" + output + \"and this is from second function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1 = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab30406",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1.add_node(\"function_1\", function_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2292ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1.add_node(\"function_2\", function_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bc3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1.add_edge(\"function_1\", \"function_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b33928",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1.set_entry_point(\"function_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_1.set_finish_point(\"function_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_1 = workflow_1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app_1.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_1.invoke(\"Hi, This is a new day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776767d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = \"Hi, This is a new day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62080e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in app_1.stream(input_1):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Here is output from {key}\")\n",
    "        print(\"------------\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d49f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_4(input):\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "    response = model.invoke(input).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_5(input):\n",
    "    upper_case = input.upper()\n",
    "    return upper_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2 = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0902c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2.add_node(\"llm\", function_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f583a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2.add_node(\"upper_case\", function_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2.add_edge(\"llm\", \"upper_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d208df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2.set_entry_point(\"llm\")\n",
    "workflow_2.set_finish_point(\"upper_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_2 = workflow_2.compit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5102e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app_2.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2 = \"What is LangGraph?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_2.invoke(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in app_2.stream(input_2):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Here is output from {key}\")\n",
    "        print(\"------------\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650e2e3",
   "metadata": {},
   "source": [
    "### Creating our own output token counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_6(input):\n",
    "    token = input.split()\n",
    "    token_number = len(token)\n",
    "    token_number =  f\"Total token number is {token_number}\"\n",
    "    return token_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dec25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow3=Graph()\n",
    "workflow3.add_node(\"11m\", function_4)\n",
    "workflow3.add_node(\"token_counter\", function_6)\n",
    "workflow3.add_edge(\"11m\", \"token_counter\")\n",
    "workflow3.set_entry_point(\"11m\")\n",
    "workflow3.set_finish_point(\"token_counter\")\n",
    "app3=workflow3.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66404b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app3.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980467e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app3.invoke(input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36a385",
   "metadata": {},
   "source": [
    "# Integrating RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"../data\", glob=\"./*.txt\", loader_cls=TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents=new_docs, embedding=embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is meta llama3?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query=query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91922147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60006a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1_for_rag(AgentState):\n",
    "    message = AgentState[\"messages\"]\n",
    "\n",
    "    question = message[-1]\n",
    "\n",
    "    complete_prompt = \"Your task is to provide only the briegf based on the user query. \\\n",
    "        Don't include too much reasoning. Follow user query:  \" + question\n",
    "    \n",
    "    response = model.invoke(complete_prompt)\n",
    "\n",
    "    AgentState[\"messages\"].append(response.content) # Appending LLM call response to the AgentState\n",
    "\n",
    "    #print(AgentState)\n",
    "\n",
    "    return AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2_for_rag(AgentState):\n",
    "    messages = AgentState[\"messages\"]\n",
    "    question = messages[0] # Fetching the user question\n",
    "\n",
    "    template = \"\"\"\"Answer the questions based only on the following context\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"question\": retriever, \"question\":RunnablePassthrough()}\n",
    "\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = retrieval_chain.invoke(question)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d658ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LangChain Graph\n",
    "workflow_for_rag = Graph()\n",
    "workflow_for_rag.add_node(\"LLM\", function_1_for_rag)\n",
    "workflow_for_rag.add_node(\"RAGTool\", function_2_for_rag)\n",
    "workflow_for_rag.add_edge(\"LLM\", \"RAGTool\")\n",
    "workflow_for_rag.set_entry_point(\"LLM\")\n",
    "workflow_for_rag.set_finish_point(\"RAGTool\")\n",
    "\n",
    "app_for_rag = workflow_for_rag.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app_for_rag.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_rag = {\"messages\":[\"tell me about llama3 model\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee745932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in app_for_rag.stream(input_for_rag):\n",
    "    #stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Here is output from node {key}: \")\n",
    "        print(\"------------\")\n",
    "        print(value)\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff708c",
   "metadata": {},
   "source": [
    "# Trying different workflow (LLM vs RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_2 = DirectoryLoader(\"../data\", glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "docs_2 = loader.load()\n",
    "\n",
    "new_docs_2 = text_splitter.split_documents(docs_2)\n",
    "doc_strings_2 = [doc.page_content for doc in new_docs_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_2 = Chroma.from_documents(new_docs_2, embeddings)\n",
    "retriever_2 = db_2.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"Tell me about USA Industrial Growth\"\n",
    "docs_2 = retriever_2.get_relevant_documents(query_1)\n",
    "print(docs_2[0].metadata)\n",
    "print(docs_2[0].page_content)\n",
    "\n",
    "for doc in docs_2:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f982f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The 'messages' field should be a sequence of strings, and we annotate it with 'operator.add'\n",
    "    # This implies we might want to 'add' new messages to the sequence later\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSelectionParser(BaseModel):\n",
    "    topic: str = Field(description=\"Selected Topic\")\n",
    "    Reasoning : str = Field(description=\"Reasoning behind topic selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c16168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1_for_comparing(state):\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[-1]\n",
    "    print(question)\n",
    "\n",
    "    template = \"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [USA, Not Related].\n",
    "    Only respond with the category name and nothing else:\n",
    "\n",
    "    user query: {question}\n",
    "    {format_instructions}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        partial_variables={\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        }\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    response = chain.invoke({\"question\": question, \"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37239bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print(\"-> Router ->\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    #last_message = last_message.upper()\n",
    "\n",
    "    if \"USA\" in last_message:\n",
    "        return \"RAG Call\"\n",
    "    \n",
    "    else:\n",
    "        return \"LLM Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_for_rag(state):\n",
    "    print(\"-> Calling RAG ->\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0] # Fetching the user question\n",
    "    print(question)\n",
    "\n",
    "    template = \"\"\"\n",
    "\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    retrieval_cahin = (\n",
    "        {\"Content\": retriever, \"question\":RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = retrieval_cahin.invoke(question)\n",
    "\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0125ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_for_llm(state):\n",
    "    print(\"-> Calling LLM ->\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0] # Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"\"\"\n",
    "    Answer the following question with your knowledge of the real world. Following is the user question\n",
    "    \"\"\" + question\n",
    "\n",
    "    response = model.invoke(complete_query)\n",
    "\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_for_llm_ray = StateGraph(AgentState) ## StateGraph with AgentState\n",
    "workflow_for_llm_ray.add_node(\"agent\", function_1_for_comparing)\n",
    "workflow_for_llm_ray.add_node(\"RAG\", function_for_rag)\n",
    "workflow_for_llm_ray.add_node(\"LLM\", function_for_llm)\n",
    "workflow_for_llm_ray.set_entry_point(\"agent\")\n",
    "\n",
    "workflow_for_llm_ray.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"RAG Call\": \"RAG\",\n",
    "        \"LLM Call\": \"LLM\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_for_llm_ray.add_edge(\"RAG\", END)\n",
    "workflow_for_llm_ray.add_edge(\"LLM\", END)\n",
    "app_for_llm_rag = workflow_for_llm_ray.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app_for_llm_rag.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc439f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_llm_rag = {\"messages\": [\"Tell me about USA industrial growth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b77fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = app_for_llm_rag.invoke(input_for_llm_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a48a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50871d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
